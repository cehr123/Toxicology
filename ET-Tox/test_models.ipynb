{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f30629-77ed-42ee-be58-aa761e3cc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics import AUROC\n",
    "# from torchmetrics.classification import BinaryAveragePrecision\n",
    "\n",
    "# import os\n",
    "# from os.path import basename, dirname, join, exists\n",
    "# import pickle\n",
    "# import time\n",
    "# import glob\n",
    "# import argparse\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from torchmdnet import datasets#, attention_weights\n",
    "# from torchmdnet.models.model import load_model\n",
    "# from torchmdnet.utils import make_splits\n",
    "# from torchmdnet.data import Subset\n",
    "# from torch_geometric.data import DataLoader\n",
    "# from torch_scatter import scatter\n",
    "# from matplotlib import pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from torchmetrics.functional.classification import binary_average_precision\n",
    "# from torch.nn.functional import mse_loss, l1_loss\n",
    "\n",
    "# def rmse(pred, target):\n",
    "#     return torch.sqrt(mse_loss(pred, target))\n",
    "\n",
    "# def null_model(predicted, ground_truth, final_test=False):\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     import numpy as np\n",
    "#     n_tasks = ground_truth.shape[1]\n",
    "#     ground_truth_np = ground_truth.cpu().numpy()\n",
    "#     predicted_np = predicted.cpu().numpy()\n",
    "#     auc = []\n",
    "#     auc_dict = {i: float for i in range(n_tasks)}\n",
    "#     for i in range(n_tasks):\n",
    "#         if np.any(ground_truth_np[:, i] == 0) and np.any(ground_truth_np[:, i] == 1):\n",
    "#             auroc = AUROC(task='binary', ignore_index=-100)\n",
    "#             auc.append(auroc(torch.zeros_like(ground_truth[:, i])+1., ground_truth[:, i]))\n",
    "#             auc_dict[i] = auc\n",
    "#         else:\n",
    "#             continue\n",
    "#     if final_test:\n",
    "#         return auc_dict, sum(auc) / len(auc)\n",
    "\n",
    "#     return sum(auc) / len(auc)\n",
    "\n",
    "\n",
    "# def null_model_pr(predicted, ground_truth, final_test=False):\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     import numpy as np\n",
    "#     n_tasks = ground_truth.shape[1]\n",
    "#     ground_truth_np = ground_truth.cpu().numpy()\n",
    "#     predicted_np = predicted.cpu().numpy()\n",
    "#     auc = []\n",
    "#     auc_dict = {i: float for i in range(n_tasks)}\n",
    "#     for i in range(n_tasks):\n",
    "#         if np.any(ground_truth_np[:, i] == 0) and np.any(ground_truth_np[:, i] == 1):\n",
    "#             average_precision = BinaryAveragePrecision(threshold=None, ignore_index=-100)\n",
    "#             auc.append(average_precision(torch.zeros_like(ground_truth[:, i]).float()+1., ground_truth[:, i]))\n",
    "#             auc_dict[i] = auc\n",
    "#         else:\n",
    "#             continue\n",
    "#     if final_test:\n",
    "#         return auc_dict, sum(auc) / len(auc)\n",
    "\n",
    "#     return sum(auc) / len(auc)\n",
    "\n",
    "# def multitask_prauc(predicted, ground_truth, final_test=False):\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     import numpy as np\n",
    "#     n_tasks = ground_truth.shape[1]\n",
    "#     ground_truth_np = ground_truth.cpu().numpy()\n",
    "#     predicted_np = predicted.cpu().numpy()\n",
    "#     auc = []\n",
    "#     auc_dict = {i: float for i in range(n_tasks)}\n",
    "#     for i in range(n_tasks):\n",
    "#         if np.any(ground_truth_np[:, i] == 0) and np.any(ground_truth_np[:, i] == 1):\n",
    "#             average_precision = BinaryAveragePrecision(threshold=None, ignore_index=-100)\n",
    "#             auc.append(average_precision(predicted[:, i], ground_truth[:, i]))\n",
    "#             auc_dict[i] = auc\n",
    "#         else:\n",
    "#             continue\n",
    "#     if final_test:\n",
    "#         return auc_dict, sum(auc) / len(auc)\n",
    "\n",
    "#     return sum(auc) / len(auc)\n",
    "\n",
    "# def multitask_auc(predicted, ground_truth, final_test=False):\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     import numpy as np\n",
    "#     n_tasks = ground_truth.shape[1]\n",
    "#     ground_truth_np = ground_truth.cpu().numpy()\n",
    "#     predicted_np = predicted.cpu().numpy()\n",
    "#     auc = []\n",
    "#     auc_dict = {i: float for i in range(n_tasks)}\n",
    "#     for i in range(n_tasks):\n",
    "#         if np.any(ground_truth_np[:, i] == 0) and np.any(ground_truth_np[:, i] == 1):\n",
    "#             auroc = AUROC(task='binary', ignore_index=-100)\n",
    "#             auc.append(auroc(predicted[:, i], ground_truth[:, i]))\n",
    "#             auc_dict[i] = auc\n",
    "#         else:\n",
    "#             continue\n",
    "#     #import pdb; pdb.set_trace()\n",
    "#     if final_test:\n",
    "#         return auc_dict, sum(auc) / len(auc)\n",
    "#     return sum(auc) / len(auc)\n",
    "\n",
    "# dataset_name = \"dili\"\n",
    "\n",
    "# model_path = f\"./models/{dataset_name}.ckpt\"\n",
    "\n",
    "# dataset = \"TDCTox\"\n",
    "# dataset_arg = {\"num_conformers\": 1, \"conformer\": \"best\", \"dataset\": dataset_name}\n",
    "# dataset_root = \"./data/TDCTox\"\n",
    "# dataset_split = \"scaffold\"\n",
    "# splits_path = f\"./data/TDCTox/splits/{dataset_name}_split_1_scaffold.npz\"\n",
    "\n",
    "# #dataset = \"MoleculeNet\"\n",
    "# #dataset_arg = {\"num_conformers\": 1, \"conformer\": \"best\", \"data_version\": \"geom\", \"dataset\": dataset_name}\n",
    "# #dataset_root = \"./data/MoleculeNet\"\n",
    "# #dataset_split = \"scaffold\"\n",
    "# #splits_path = f\"./data/MoleculeNet/splits/{dataset_name}_seed1_confs1_scaffold.npz\"\n",
    "\n",
    "\n",
    "# data = getattr(datasets, dataset)(dataset_root, **dataset_arg)\n",
    "\n",
    "# splits = np.load(splits_path)\n",
    "# data = DataLoader(torch.utils.data.Subset(data, splits[\"idx_test\"]), batch_size=1, num_workers=6)\n",
    "\n",
    "# # load model\n",
    "# print(\"loading model\")\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "# model = load_model(model_path, device=device).eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318fe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded preprocessed MMAP dataset from ./data/MoleculeNet/processed\n",
      "[INFO] Loading model from ./output_dir/tox21.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 757/757 [01:54<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test AUROC: 0.9462\n",
      "✅ Test Average Precision: 0.7160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision\n",
    "from torch_geometric.data import DataLoader\n",
    "from torchmdnet import datasets\n",
    "from torchmdnet.models.model import load_model\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "dataset_name = \"tox21\"\n",
    "model_path = \"./output_dir/tox21.ckpt\"\n",
    "dataset_root = \"./data/MoleculeNet\"\n",
    "splits_path = \"./data/MoleculeNet/splits/tox21_seed1_confs1_scaffold.npz\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "dataset = datasets.MoleculeNet(\n",
    "    root=dataset_root,\n",
    "    num_conformers=1,\n",
    "    conformer=\"best\",\n",
    "    data_version=\"geom\",\n",
    "    dataset=dataset_name,\n",
    ")\n",
    "\n",
    "splits = np.load(splits_path)\n",
    "test_subset = torch.utils.data.Subset(dataset, splits[\"idx_test\"])\n",
    "test_loader = DataLoader(test_subset, batch_size=1, num_workers=0)\n",
    "\n",
    "# ========== LOAD MODEL ==========\n",
    "print(f\"[INFO] Loading model from {model_path}\")\n",
    "model = load_model(model_path, device=device)\n",
    "model.eval()\n",
    "\n",
    "# ========== METRICS ==========\n",
    "auroc_metric = BinaryAUROC()\n",
    "ap_metric = BinaryAveragePrecision()\n",
    "\n",
    "# ========== EVALUATION LOOP ==========\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.z, batch.pos, batch.batch)\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        preds = torch.sigmoid(out).squeeze(0)  # remove extra dim\n",
    "\n",
    "        labels = batch.tox_labels.squeeze(0)\n",
    "        valid_mask = labels != -100  # only valid assay targets\n",
    "\n",
    "        preds, labels = preds[valid_mask], labels[valid_mask]\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "\n",
    "\n",
    "preds = torch.cat(all_preds)\n",
    "labels = torch.cat(all_labels)\n",
    "\n",
    "# ========== COMPUTE METRICS ==========\n",
    "auroc = auroc_metric(preds, labels.int())\n",
    "ap = ap_metric(preds, labels.int())\n",
    "\n",
    "print(f\"\\n✅ Test AUROC: {auroc:.4f}\")\n",
    "print(f\"✅ Test Average Precision: {ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fe17d7-bc03-45f5-8f58-792fc1eafe56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m targets_test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m energies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdata\u001b[49m):\n\u001b[1;32m      8\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(batch\u001b[38;5;241m.\u001b[39mtox_labels\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      9\u001b[0m     targets_test \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtox_labels\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "preds_test = []\n",
    "targets_test = []\n",
    "energies = []\n",
    "\n",
    "\n",
    "for batch in tqdm(data):\n",
    "    labels.append(batch.tox_labels.numpy())\n",
    "    targets_test += batch.tox_labels.cpu()\n",
    "    energies.append(batch.y)\n",
    "    \n",
    "    z, pos, batch, Q = batch.z.to(device), batch.pos.to(device), batch.batch.to(device), batch.Q.to(device)\n",
    "    pred, deriv = model(z, pos, batch, Q=Q)\n",
    "    preds_test += pred.detach().cpu()\n",
    "    \n",
    "targets = torch.stack(targets_test)\n",
    "preds = torch.stack(preds_test)\n",
    "\n",
    "if dataset_name == \"ld50\":\n",
    "    print(f\"The MAE is {l1_loss(preds, targets)}\")\n",
    "    print(f\"The RMSE is {rmse(preds, targets)}\")\n",
    "    print(f\"The null-model RMSE is {rmse(torch.tensor([2.54]).repeat_interleave(len(targets)).unsqueeze(1), targets)}\")\n",
    "else:\n",
    "    aucs, mean_auc = multitask_auc(preds, targets.long(), final_test=True)\n",
    "    aucs_pr, mean_auc_pr = multitask_prauc(preds, targets.long(), final_test=True)\n",
    "    null_aucs, mean_null_aucs = null_model(preds, targets.long(), final_test=True)\n",
    "    null_aucs_pr, mean_null_aucs_pr = null_model_pr(preds, targets.long(), final_test=True)\n",
    "\n",
    "    print(f\"\\n The ROC-AUC is: {mean_auc}\\n\")\n",
    "    print(f\"\\n The PR-AUC is: {mean_auc_pr}\\n\")\n",
    "    print(f\"\\n The null model is: {mean_null_aucs}\\n\")\n",
    "    print(f\"\\n The PR null model is: {mean_null_aucs_pr}\\n\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
